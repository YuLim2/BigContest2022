{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cdfa30",
   "metadata": {},
   "source": [
    "## 제주도 관광 이슈 크롤링\n",
    "네이버 뉴스에서 해당하는 달의 관광 이슈 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971b8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssl import SSLError\n",
    "from urllib import parse\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import socket\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f984fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(query, save_as, begin, end, sort=0, field=1, delay=0.5, timeout=30, page_limit=50):\n",
    "    '''\n",
    "    :param query: 네이버 '뉴스'란에서 검색할 검색어\n",
    "    :param save_as: 검색 결과 저장 경로\n",
    "    :param begin: '기간' -> 검색 기간 시작\n",
    "    :param end: '기간' -> 검색 기간 끝\n",
    "    :param sort: '유형' -> 0(관련도순) 1(최신순) 2(오래된순)\n",
    "    :param field: '영역' -> 0(전체) 1(제목)\n",
    "    :param delay: (옵션) 검색 리퀘스트 간격 (초)\n",
    "    :param timeout: (옵션) 타임아웃 시 기다릴 시간 (초)\n",
    "    :param page_limit: (옵션) 검색 결과에서 몇 페이지까지 갈 것인지 결정\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # prerequisite\n",
    "    df = pd.DataFrame(columns=['link', 'title', 'date', 'article'])\n",
    "\n",
    "    # index settings\n",
    "    # a single pages includes 10 news, starting from page 1 (index 1~10)\n",
    "    current_index = 1\n",
    "    max_index = 2\n",
    "\n",
    "    while (current_index <= max_index) and (1 + current_index // 10 <= page_limit):\n",
    "        print('\\n' + 'crawling... %s (current_page / max_page %i/%i)' % (query, 1 + current_index // 10, 1 + max_index // 10))\n",
    "        url = make_url(query, sort, field, begin, end, current_index)\n",
    "        print('making url', url)\n",
    "\n",
    "        print('making beautifulsoup object from html')\n",
    "        bsobj = make_bsobj(url, delay, timeout, trial=10)\n",
    "\n",
    "        if bsobj is None:\n",
    "            continue\n",
    "        print('extracting naver news urls from bsobj')\n",
    "        naver_news_urls = make_naver_news_urls(bsobj)\n",
    "        naver_news_title = get_naver_news_title(bsobj)\n",
    "        print(naver_news_urls)\n",
    "        print(naver_news_title)\n",
    "        naver_news_articles = []\n",
    "        for i in range(len(naver_news_urls)):\n",
    "            print('\\topening:', naver_news_urls[i])\n",
    "            news_bsobj = BeautifulSoup(naver_news_urls[i], 'html.parser')\n",
    "            \n",
    "            naver_news_article = get_naver_news_article(news_bsobj)\n",
    "            naver_news_articles.append(naver_news_article)\n",
    "\n",
    "#             date, article, title, newspaper = attributes\n",
    "        df = pd.DataFrame([ x for x in zip(naver_news_urls, naver_news_title)])\n",
    "        print(df)\n",
    "        print('saving updated df')\n",
    "#         df = df.sort_values(by=['date'])\n",
    "#         df.to_excel(save_as, engine='xlsxwriter')\n",
    "        current_index += 10\n",
    "        max_index = get_max_index(bsobj)\n",
    "        if max_index is None:\n",
    "            break\n",
    "    return naver_news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2b332ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "crawling... 제주도관광 (current_page / max_page 1/1)\n",
      "making url https://search.naver.com/search.naver?&where=news&query=%EC%A0%9C%EC%A3%BC%EB%8F%84%EA%B4%80%EA%B4%91&sort=0&field=1&ds=2015.01.01&de=2015.01.05&nso=so:r,p:from20150101to20150105&start=1&refresh_start=0\n",
      "making beautifulsoup object from html\n",
      "extracting naver news urls from bsobj\n",
      "['http://www.wowtv.co.kr/newscenter/news/view.asp?bcode=T30001000&artid=A201501040200', 'http://www.asiatoday.co.kr/view.php?key=20150104010001014']\n",
      "['부영그룹, 관광레저산업 전략사업 설정...제주도 시내면세점 추진', '제주도, 올해 1300만 관광객 유치목표...\"질적성장 도민소득 ↑\"']\n",
      "\topening: http://www.wowtv.co.kr/newscenter/news/view.asp?bcode=T30001000&artid=A201501040200\n",
      "\topening: http://www.asiatoday.co.kr/view.php?key=20150104010001014\n",
      "                                                   0  \\\n",
      "0  http://www.wowtv.co.kr/newscenter/news/view.as...   \n",
      "1  http://www.asiatoday.co.kr/view.php?key=201501...   \n",
      "\n",
      "                                        1  \n",
      "0     부영그룹, 관광레저산업 전략사업 설정...제주도 시내면세점 추진  \n",
      "1  제주도, 올해 1300만 관광객 유치목표...\"질적성장 도민소득 ↑\"  \n",
      "saving updated df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jin-yulim/miniforge3/envs/tf25/lib/python3.8/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = crawl('제주도관광', 'test.xlsx', '2015.01.01', '2015.01.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f53344fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.wowtv.co.kr/newscenter/news/view.asp?bcode=T30001000&artid=A201501040200',\n",
       " 'http://www.asiatoday.co.kr/view.php?key=20150104010001014']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f98e7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url(query, sort, field, begin, end, page):\n",
    "    url = \"https://search.naver.com/search.naver?&where=news&query=\" + parse.quote(query)\n",
    "    url += \"&sort=%i\" % sort\n",
    "    url += \"&field=%i\" % field\n",
    "    url += \"&ds=\" + begin + \"&de=\" + end\n",
    "    url += \"&nso=so:r,p:\"\n",
    "    url += \"from\" + begin.replace(\".\", \"\") + \"to\" + end.replace(\".\", \"\")\n",
    "    url += \"&start=\" + str(page)\n",
    "    url += \"&refresh_start=0\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "260ef478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bsobj(url, delay=0.5, timeout=30, trial=10):\n",
    "    ua = UserAgent(verify_ssl=False)\n",
    "    count = 0\n",
    "\n",
    "    while count < trial:\n",
    "        try:\n",
    "            time.sleep(delay + random.random())\n",
    "            html = urlopen(Request(url=url, headers={'User-Agent': ua.random}), timeout=timeout)\n",
    "            bsobj = BeautifulSoup(html, 'html.parser')\n",
    "            return bsobj\n",
    "        except (URLError, SSLError, socket.timeout) as e:\n",
    "            print('(Error)', e)\n",
    "            print('reloading...')\n",
    "            count += 1\n",
    "            time.sleep(timeout)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2bf7f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_naver_news_urls(bsobj):\n",
    "    return [link['href'] for link in bsobj.find_all('a', 'news_tit')]\n",
    "\n",
    "def get_naver_news_title(bsobj):\n",
    "    return [link.text for link in bsobj.find_all('a', 'news_tit')]\n",
    "\n",
    "def get_naver_news_article(news_bsobj):\n",
    "    return news_bsobj.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "478ea923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(bsobj):\n",
    "    def _get_title(bsobj):\n",
    "        ls = []\n",
    "        title = bsobj.find('a', 'news_tit').text\n",
    "        ls.append(title)\n",
    "        print(ls)\n",
    "        return title\n",
    "\n",
    "    def _get_article(bsobj):\n",
    "        article = bsobj.select('#articleBodyContents')[0].text\n",
    "        article = article.encode('utf-8', 'replace').decode()\n",
    "        return article\n",
    "\n",
    "    def _get_date(news_bsobj):\n",
    "        splits = bsobj.select('.t11')[0].text.split(' ')\n",
    "        date = splits[0] + ' ' + splits[2]\n",
    "        date = datetime.datetime.strptime(date, '%Y.%m.%d. %H:%M')\n",
    "        date += datetime.timedelta(hours=12 * int(splits[1] == '오후'))\n",
    "        print(date)\n",
    "        return date\n",
    "\n",
    "    def _get_newspaper(bsobj):\n",
    "        newspaper = bsobj.find(\"div\", class_=\"press_logo\").find('img', alt=True).get('alt')\n",
    "        return newspaper\n",
    "\n",
    "    try:\n",
    "        return _get_date(bsobj), _get_article(bsobj), _get_title(bsobj), _get_newspaper(bsobj)\n",
    "    except IndexError:\n",
    "        print('(Error) crawling failed (maybe url is redirected to somewhere else)')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4a70bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(bsobj):\n",
    "    paging = bsobj.find(\"div\", {\"class\": \"sc_page_inner\"})\n",
    "    if not paging:\n",
    "        print('(WARNING!) no results found')\n",
    "        return None\n",
    "\n",
    "    atags = paging.find_all('a')\n",
    "    if not atags:\n",
    "        print('(WARNING!) there is only one page')\n",
    "        return None\n",
    "\n",
    "    return max([int(atag[\"href\"].split('start=')[1]) for atag in atags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a0ba3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    # Argument configuration\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--query', type=str, required=True, help='query to search on NAVER')\n",
    "    parser.add_argument('--begin', type=str, required=True, help='crawling begin point (%%Y.%%m.%%d format)')\n",
    "    parser.add_argument('--end', type=str, required=True, help='crawling end point (%%Y.%%m.%%d format)')\n",
    "    parser.add_argument('--save_as', type=str, default='test2.xlsx', help='excel save path')\n",
    "    parser.add_argument('--sort', type=int, default=0, help='search result sorting: 0(relevant), 1(newest), 2(oldest)')\n",
    "    parser.add_argument('--field', type=int, default=1, help='search field: 0(all), 1(title)')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac1da847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "crawling... 제주도관광 (current_page / max_page 1/1)\n",
      "making url https://search.naver.com/search.naver?&where=news&query=%EC%A0%9C%EC%A3%BC%EB%8F%84%EA%B4%80%EA%B4%91&sort=0&field=1&ds=2015.01.01&de=2015.01.05&nso=so:r,p:from20150101to20150105&start=1&refresh_start=0\n",
      "making beautifulsoup object from html\n",
      "extracting naver news urls from bsobj\n",
      "['http://www.wowtv.co.kr/newscenter/news/view.asp?bcode=T30001000&artid=A201501040200', 'http://www.asiatoday.co.kr/view.php?key=20150104010001014']\n",
      "['부영그룹, 관광레저산업 전략사업 설정...제주도 시내면세점 추진', '제주도, 올해 1300만 관광객 유치목표...\"질적성장 도민소득 ↑\"']\n",
      "\topening: http://www.wowtv.co.kr/newscenter/news/view.asp?bcode=T30001000&artid=A201501040200\n",
      "\topening: http://www.asiatoday.co.kr/view.php?key=20150104010001014\n",
      "                                                   0  \\\n",
      "0  http://www.wowtv.co.kr/newscenter/news/view.as...   \n",
      "1  http://www.asiatoday.co.kr/view.php?key=201501...   \n",
      "\n",
      "                                        1  \n",
      "0     부영그룹, 관광레저산업 전략사업 설정...제주도 시내면세점 추진  \n",
      "1  제주도, 올해 1300만 관광객 유치목표...\"질적성장 도민소득 ↑\"  \n",
      "saving updated df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jin-yulim/miniforge3/envs/tf25/lib/python3.8/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = crawl('제주도관광', 'test.xlsx', '2015.01.01', '2015.01.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a9ba2f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.wowtv.co.kr/newscenter/news/view.asp?bcode=T30001000&artid=A201501040200',\n",
       " 'http://www.asiatoday.co.kr/view.php?key=20150104010001014']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8651c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
