{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cdfa30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 제주도 관광 이슈 크롤링\n",
    "네이버 뉴스에서 해당하는 달의 관광 이슈 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b8c16",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ssl import SSLError\n",
    "from urllib import parse\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import socket\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f984fd9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def crawl(query, save_as, begin, end, sort=0, field=1, delay=0.5, timeout=30, page_limit=50):\n",
    "    '''\n",
    "    :param query: 네이버 '뉴스'란에서 검색할 검색어\n",
    "    :param save_as: 검색 결과 저장 경로\n",
    "    :param begin: '기간' -> 검색 기간 시작\n",
    "    :param end: '기간' -> 검색 기간 끝\n",
    "    :param sort: '유형' -> 0(관련도순) 1(최신순) 2(오래된순)\n",
    "    :param field: '영역' -> 0(전체) 1(제목)\n",
    "    :param delay: (옵션) 검색 리퀘스트 간격 (초)\n",
    "    :param timeout: (옵션) 타임아웃 시 기다릴 시간 (초)\n",
    "    :param page_limit: (옵션) 검색 결과에서 몇 페이지까지 갈 것인지 결정\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # prerequisite\n",
    "    df = pd.DataFrame(columns=['link', 'title', 'date', 'article'])\n",
    "\n",
    "    # index settings\n",
    "    # a single pages includes 10 news, starting from page 1 (index 1~10)\n",
    "    current_index = 1\n",
    "    max_index = 2\n",
    "\n",
    "    while (current_index <= max_index) and (1 + current_index // 10 <= page_limit):\n",
    "        url = make_url(query, sort, field, begin, end, current_index)\n",
    "        bsobj = make_bsobj(url, delay, timeout, trial=10)\n",
    "\n",
    "        if bsobj is None:\n",
    "            continue\n",
    "        naver_news_urls = make_naver_news_urls(bsobj)\n",
    "        naver_news_title = get_naver_news_title(bsobj)\n",
    "        naver_news_articles = []\n",
    "        for i in range(len(naver_news_urls)):\n",
    "            url = naver_news_urls[i]\n",
    "            news_bsobj = BeautifulSoup(url, 'lxml')\n",
    "            \n",
    "            naver_news_article = get_naver_news_article(url, news_bsobj)\n",
    "            naver_news_articles.append(naver_news_article)\n",
    "\n",
    "#             date, article, title, newspaper = attributes\n",
    "        df = pd.DataFrame([ x for x in zip(naver_news_title, naver_news_articles)])\n",
    "        df.columns = ['naver_news_title', 'naver_news_articles']\n",
    "#         df = df.sort_values(by=['date'])\n",
    "        df.to_excel(save_as, engine='xlsxwriter')\n",
    "        current_index += 10\n",
    "    \n",
    "        max_index = get_max_index(bsobj)\n",
    "        if max_index is None:\n",
    "            break\n",
    "        if len(df) > 50:\n",
    "            break\n",
    "        \n",
    "    print(f'{begin}_complete!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db9def4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_naver_news_urls(bsobj):\n",
    "    return [link['href'] for link in bsobj.find_all('a', 'news_tit')]\n",
    "\n",
    "def get_naver_news_title(bsobj):\n",
    "    return [link.text for link in bsobj.find_all('a', 'news_tit')]\n",
    "\n",
    "def get_naver_news_article(url, news_bsobj):\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        return text\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98e7aa6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_url(query, sort, field, begin, end, page):\n",
    "    url = \"https://search.naver.com/search.naver?&where=news&query=\" + parse.quote(query)\n",
    "    url += \"&sort=%i\" % sort\n",
    "    url += \"&field=%i\" % field\n",
    "    url += \"&ds=\" + begin + \"&de=\" + end\n",
    "    url += \"&nso=so:r,p:\"\n",
    "    url += \"from\" + begin.replace(\".\", \"\") + \"to\" + end.replace(\".\", \"\")\n",
    "    url += \"&start=\" + str(page)\n",
    "    url += \"&refresh_start=0\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260ef478",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_bsobj(url, delay=0.5, timeout=30, trial=10):\n",
    "    ua = UserAgent(verify_ssl=False)\n",
    "    count = 0\n",
    "\n",
    "    while count < trial:\n",
    "        try:\n",
    "            time.sleep(delay + random.random())\n",
    "            html = urlopen(Request(url=url, headers={'User-Agent': ua.random}), timeout=timeout)\n",
    "            bsobj = BeautifulSoup(html, 'lxml')\n",
    "            return bsobj\n",
    "        except (URLError, SSLError, socket.timeout) as e:\n",
    "            print('(Error)', e)\n",
    "            print('reloading...')\n",
    "            count += 1\n",
    "            time.sleep(timeout)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a70bc0a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_index(bsobj):\n",
    "    paging = bsobj.find(\"div\", {\"class\": \"sc_page_inner\"})\n",
    "    if not paging:\n",
    "        print('(WARNING!) no results found')\n",
    "        return None\n",
    "\n",
    "    atags = paging.find_all('a')\n",
    "    if not atags:\n",
    "        print('(WARNING!) there is only one page')\n",
    "        return None\n",
    "\n",
    "    return max([int(atag[\"href\"].split('start=')[1]) for atag in atags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ba3b5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    # Argument configuration\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--query', type=str, required=True, help='query to search on NAVER')\n",
    "    parser.add_argument('--begin', type=str, required=True, help='crawling begin point (%%Y.%%m.%%d format)')\n",
    "    parser.add_argument('--end', type=str, required=True, help='crawling end point (%%Y.%%m.%%d format)')\n",
    "    parser.add_argument('--sort', type=int, default=0, help='search result sorting: 0(relevant), 1(newest), 2(oldest)')\n",
    "    parser.add_argument('--field', type=int, default=1, help='search field: 0(all), 1(title)')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def ran_num(n):\n",
    "    ls = []\n",
    "    num = random.randint(1, n)\n",
    "    while n in ls :\n",
    "        num = random.randint(1, n)\n",
    "        ls.append(num)\n",
    "    return num"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def saving_data(start_year, end_year):\n",
    "    for i in range(start_year, end_year + 1):\n",
    "        for j in range(1, 13):\n",
    "            if j < 10:\n",
    "                j = '0' + str(j)\n",
    "            df = crawl('제주도관광', f'./news_data/{i}_{j}.xlsx', f'{i}.{j}.01', f'{i}.{j}.30')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015.01.01_complete!\n",
      "2015.02.01_complete!\n",
      "2015.03.01_complete!\n",
      "2015.04.01_complete!\n",
      "2015.05.01_complete!\n",
      "(WARNING!) no results found\n",
      "2015.06.01_complete!\n",
      "2015.07.01_complete!\n",
      "2015.08.01_complete!\n",
      "2015.09.01_complete!\n",
      "2015.10.01_complete!\n",
      "2015.11.01_complete!\n",
      "2015.12.01_complete!\n",
      "2016.01.01_complete!\n",
      "2016.02.01_complete!\n",
      "2016.03.01_complete!\n",
      "2016.04.01_complete!\n",
      "2016.05.01_complete!\n",
      "2016.06.01_complete!\n",
      "2016.07.01_complete!\n",
      "2016.08.01_complete!\n",
      "2016.09.01_complete!\n",
      "2016.10.01_complete!\n",
      "2016.11.01_complete!\n",
      "2016.12.01_complete!\n",
      "2017.01.01_complete!\n",
      "2017.02.01_complete!\n",
      "2017.03.01_complete!\n",
      "2017.04.01_complete!\n",
      "2017.05.01_complete!\n",
      "2017.06.01_complete!\n",
      "2017.07.01_complete!\n",
      "2017.08.01_complete!\n",
      "2017.09.01_complete!\n",
      "2017.10.01_complete!\n",
      "2017.11.01_complete!\n",
      "2017.12.01_complete!\n",
      "2018.01.01_complete!\n",
      "2018.02.01_complete!\n",
      "2018.03.01_complete!\n",
      "2018.04.01_complete!\n",
      "2018.05.01_complete!\n",
      "2018.06.01_complete!\n",
      "2018.07.01_complete!\n",
      "2018.08.01_complete!\n",
      "2018.09.01_complete!\n",
      "2018.10.01_complete!\n",
      "2018.11.01_complete!\n",
      "2018.12.01_complete!\n",
      "2019.01.01_complete!\n",
      "2019.02.01_complete!\n",
      "2019.03.01_complete!\n",
      "2019.04.01_complete!\n",
      "2019.05.01_complete!\n",
      "2019.06.01_complete!\n",
      "(WARNING!) no results found\n",
      "2019.07.01_complete!\n",
      "2019.08.01_complete!\n",
      "2019.09.01_complete!\n",
      "2019.10.01_complete!\n",
      "2019.11.01_complete!\n",
      "2019.12.01_complete!\n",
      "2020.01.01_complete!\n",
      "(WARNING!) no results found\n",
      "2020.02.01_complete!\n",
      "2020.03.01_complete!\n",
      "2020.04.01_complete!\n",
      "2020.05.01_complete!\n",
      "2020.06.01_complete!\n",
      "2020.07.01_complete!\n",
      "2020.08.01_complete!\n",
      "2020.09.01_complete!\n",
      "2020.10.01_complete!\n",
      "2020.11.01_complete!\n",
      "2020.12.01_complete!\n",
      "2021.01.01_complete!\n",
      "2021.02.01_complete!\n",
      "2021.03.01_complete!\n",
      "2021.04.01_complete!\n",
      "2021.05.01_complete!\n",
      "2021.06.01_complete!\n",
      "2021.07.01_complete!\n",
      "2021.08.01_complete!\n",
      "2021.09.01_complete!\n",
      "2021.10.01_complete!\n",
      "2021.11.01_complete!\n",
      "2021.12.01_complete!\n"
     ]
    }
   ],
   "source": [
    "saving_data(2015, 2021)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}